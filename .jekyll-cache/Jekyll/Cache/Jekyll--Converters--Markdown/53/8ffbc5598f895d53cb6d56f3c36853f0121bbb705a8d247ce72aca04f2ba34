I"ÕA<h1 id="introduction-to-shannon-entropy">Introduction to Shannon Entropy</h1>

<p><em>01 Feb 2020</em></p>

<h2 id="entropy-measuring-information">Entropy (Measuring Information)</h2>

<h3 id="definition-of-shannons-entropy">Definition of Shannonâ€™s Entropy</h3>

<ul>
  <li>ä¿¡æ¯ä¸­æœ‰
    <ul>
      <li>æœ‰æ„ä¹‰ä¿¡æ¯(éå†—ä½™)</li>
      <li>æ— æ„ä¹‰ä¿¡æ¯</li>
      <li>å†—ä½™ä¿¡æ¯</li>
    </ul>
  </li>
  <li>Bitä½œä¸ºä¿¡æ¯é‡(Information)çš„è¡¡é‡æ–¹å¼
    <ul>
      <li>Bit = 0 or 1 (æ˜¯æˆ–ä¸æ˜¯, å¤©ç„¶ç­‰æ¦‚ç‡)</li>
      <li>Bit = Uncertainty divided by 2</li>
    </ul>
  </li>
  <li>å¯¹äºBitè®¡ç®—æ–¹æ³•çš„ä¸¤ç§ç›´è§‚ç†è§£æ–¹å¼
    <ul>
      <li>Nç§æƒ…å†µ(state) -&gt; logè®¡ç®— -&gt; å¾—åˆ°ä¿¡æ¯é‡ $\log_{2}^{N}$
        <ul>
          <li>å¯ä»¥è¿™æ ·ç†è§£ï¼ŒNç§æƒ…å†µç­‰å¯èƒ½(Uncertainty divided by N, N is uncertainty reduction factor)ï¼ŒæŠŠæ‰€æœ‰æƒ…å†µèµ‹äºˆäºŒè¿›åˆ¶ç¼–ç , ä¸ºäº†å®Œæ•´æè¿°æ¯ä¸€æƒ…å†µï¼Œéœ€è¦çš„äºŒè¿›åˆ¶ä½æ•°å³æ˜¯ä¼ é€’è¯¥æƒ…å†µæ‰€éœ€çš„ä¿¡æ¯é‡</li>
          <li>åœ¨æ­¤ç§ç†è§£æ–¹å¼ä¸‹
            <ul>
              <li>Nåªèƒ½ä¸ºæ­£æ•´æ•°?</li>
              <li>åªé€‚ç”¨äºå„ä¸ªæƒ…å†µ(state)&lt;å¯èƒ½æ€§/æ¦‚ç‡&gt;å‡ç­‰çš„æƒ…å†µ?</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>ç›´æ¥logåŒ–æ¦‚ç‡ (MORE GENERAL) $-log_2^{p}$
        <ul>
          <li>ä»”ç»†æ€è€ƒä¼šçŸ¥é“ï¼Œ$\text{num of yes/no questions}=\log_{2}(\text{æƒ…å†µæ•°ç›®})=\log_{2}(1/p)$</li>
          <li>ä¸è¿‡å½“éç­‰æ¦‚ç‡(ä¸”åº•æ•°é2çš„æŒ‡æ•°å€)ä»¥åŠæ¦‚ç‡å€’æ•°éæ•´æ•°çš„æƒ…å†µä¸‹ï¼Œè¯¥å®šä¹‰ä»é€‚ç”¨çš„ç†ç”±è¿˜è¦æ€è€ƒæ¸…æ¥š</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ä»logè®¡ç®—æ¥çœ‹, å¯¹äºæ¦‚ç‡è¶Šå¤§çš„äº‹ä»¶, å…¶å¯¹åº”ä¿¡æ¯bitè¶Šä½; stateè¶Šå°‘å¯¹åº”ä¿¡æ¯bitä¹Ÿè¶Šä½ã€‚åä¹‹è¶Šå¤§ã€‚æ„å‘³ç€è¶Šç¡®å®šçš„äº‹æƒ…bitè¶Šä½(ä¿¡æ¯é‡è¶Šå°‘)ï¼Œåä¹‹è¶Šå¤§ã€‚</li>
  <li>Measure the average amount of information
    <ul>
      <li>ç›¸å½“äºæ•°å­¦æœŸæœ› (average number of yes/no questions we need to ask to get the correct answer, å‚è§video2, äºŒå‰æ ‘çš„åº”ç”¨)</li>
      <li>$-\sum_{i}p_{i}log_{2}(p_{i})$</li>
      <li>Thatâ€™s it, the formula calculates the entropy, which measures the uncertainty.</li>
      <li>$H(\text{p}) = -\sum_{i}p_{i}log_{2}(p_{i})$, $\text{p}$ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        <ul>
          <li>it tells how unpredictable the probability distribution is.</li>
          <li>ä¸”åœ¨è¿™é‡Œçš„ä¿¡æ¯éƒ½æ˜¯æœ‰æ„ä¹‰ä¿¡æ¯ï¼Œæ— æ„ä¹‰æˆ–å†—ä½™ä¿¡æ¯å¯ä»¥çœ‹ä½œæ˜¯ä¸ºä¼ é€’æœ‰æ„ä¹‰ä¿¡æ¯è€Œäº§ç”Ÿçš„æˆæœ¬ï¼Œä¼ é€’çš„ä¿¡æ¯çš„é›†åˆç§°ä¸ºæ¶ˆæ¯(message)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Index</th>
      <th>Weather</th>
      <th>Possibility</th>
      <th>Code</th>
      <th>Information</th>
      <th>Message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>ğŸŒ</td>
      <td>0.125</td>
      <td>000</td>
      <td>3 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>1</td>
      <td>â›…</td>
      <td>0.125</td>
      <td>001</td>
      <td>3 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>2</td>
      <td>â˜</td>
      <td>0.125</td>
      <td>010</td>
      <td>3 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>3</td>
      <td>ğŸŒ¨</td>
      <td>0.125</td>
      <td>011</td>
      <td>3 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>4</td>
      <td>ğŸŒ§</td>
      <td>0.125</td>
      <td>100</td>
      <td>3 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>5</td>
      <td>ğŸŒ©</td>
      <td>0.125</td>
      <td>101</td>
      <td>3 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>6</td>
      <td>ğŸŒª</td>
      <td>0.125</td>
      <td>110</td>
      <td>3 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>7</td>
      <td>ğŸŒ«</td>
      <td>0.125</td>
      <td>111</td>
      <td>3 bits</td>
      <td>3 bits</td>
    </tr>
  </tbody>
</table>

<p>è¡¨1</p>

<pre><code class="language-viz">graph tree1 {
    edge [color="0.700 0.200 1.000"];
    node [style=filled, color="0.650 0.200 1.000"];
    overlap=false;
    ABCD--AB[label="1/2"];
    ABCD--CD[label="1/2"];
    AB--A[label="1/4"];
    AB--B[label="1/4"];
    CD--C[label="1/4"];
    CD--D[label="1/4"];
}
</code></pre>

<p>å›¾1 (ç­‰æ¦‚ç‡ï¼Œå±‚å±‚äºŒåˆ†)</p>

<pre><code class="language-viz">graph tree2 {
    edge [color="0.700 0.200 1.000"];
    node [style=filled, color="0.650 0.200 1.000"];
    ABCD--A[label="1/2"];
    ABCD--BCD[label="1/2"];
    BCD--D[label="1/4"];
    BCD--BC[label="1/4"];
    BC--B[label="1/8"];
    BC--C[label="1/8"];
}
</code></pre>

<p>å›¾2 (éç­‰æ¦‚ç‡ï¼Œä½†å¯å±‚å±‚äºŒåˆ†)</p>

<pre><code class="language-viz">graph tree3 {
    edge [color="0.700 0.200 1.000"];
    node [style=filled, color="0.650 0.200 1.000"];
    A[label="3/5"];
    B[label="1/5"];
    C[label="1/10"];
    D[label="1/10"];
}
</code></pre>

<p>å›¾3 (éç­‰æ¦‚ç‡ï¼Œä¸å¯äºŒåˆ†? å¯ä»¥è€ƒè™‘å“ˆå¤«æ›¼æ ‘çš„æ„å»ºä¸å“ˆå¤«æ›¼ç¼–ç ï¼Œé»˜è®¤å‰ç¼€ç¼–ç ï¼Œä½†æ˜¯æ„ä¹‰ä¸ä¸ŠäºŒå›¾ä¸åŒ)</p>

<ul>
  <li>Cross-Entropy
    <ul>
      <li>å¯ä»¥ç†è§£ä¸ºä¼ é€’æ¶ˆæ¯(message)æ‰€éœ€çš„æœŸæœ›ä¿¡æ¯é‡?</li>
      <li>è¡¨2ä¸­ <script type="math/tex">\text{Entropy} = 2.23 \,\text{bits},\, \text{Cross-Entropy} = 3\,\text{bits}</script></li>
      <li>è¡¨3ä¸­ <script type="math/tex">\text{Cross-Entropy} = 2.42\,\text{bits}</script></li>
      <li><script type="math/tex">H(\text{p, q})=-\sum_{i}p_{i}\log_{2}(q_{i})$, $\text{p}$ä¸ºtrueæ¦‚ç‡åˆ†å¸ƒ, $\text{q}</script>ä¸ºpredictedæ¦‚ç‡åˆ†å¸ƒ</li>
      <li>æåŠçš„äºŒè¡¨çš„Messageåˆ—å³ä¸ºpredicted distributionï¼Œç”±Codeåˆ—å†³å®š</li>
      <li>predicted possibilityåŠ å’Œä¸ä¸€å®šä¸º1</li>
      <li>å¦‚æœé¢„æµ‹è‰¯å¥½(ä¸¤distributionç›¸ç­‰)ï¼Œåˆ™Cross-Entropyä¸Entropyè®¡ç®—æ•°å€¼ç›¸ç­‰; å¦‚æœpredicted distributionä¸true distributionå­˜åœ¨å·®å¼‚ï¼Œåˆ™Cross-Entropyå°†ä¼šæ¯”Entropyå¤§(åœ¨messageè¿›è¡Œåˆç†ç¼–ç çš„æƒ…å†µä¸‹?)
        <ul>
          <li>
            <script type="math/tex; mode=display">\text{Cross-Entropy}-\text{Entropy} \Rightarrow \text{Relative Entropy} \Rightarrow \text{Kullback-Leibler divergence}</script>
          </li>
          <li>
            <script type="math/tex; mode=display">\text{Cross-Entropy} = \text{Entropy} + \text{K-L divergence}</script>
          </li>
        </ul>
      </li>
      <li>K-L divergence:
        <ul>
          <li>
            <script type="math/tex; mode=display">D_{KL}(\text{p}||\text{q})=H(\text{p, q})-H(\text{p})=-\sum_{i}p_{i}\log_{2}(q_{i})+\sum_{i}p_{i}\log_{2}(p_{i})=\sum_{i}p_{i}[log_{2}(p_{i})-log_2(q_{i})]=\sum_{i}p_{i}\log_{2}^{\cfrac{p_{i}}{q_{i}}}</script>
          </li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>å¦ä¸€ç§å†™æ³•: $$D_{KL}(P</td>
                  <td>Â </td>
                  <td>Q)=E_{x\sim P}\left[\log \cfrac{P(x)}{Q(x)}\right]$$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>ç‰¹æ®Šæƒ…å½¢: $$D(p</td>
                  <td>Â </td>
                  <td>q)=p\log^{\cfrac{p}{q}}+(1-p)\log^{\cfrac{1-p}{1-q}}$$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>Saying: â€œKL divergence from q to pâ€</li>
          <li>Properties
            <ul>
              <li>
                <script type="math/tex; mode=display">D_{KL}(P||Q)\ge 0, \forall P, Q</script>
              </li>
              <li>
                <script type="math/tex; mode=display">D_{KL}(P||Q)=0, \text{if P and Q are equal almost surely}</script>
              </li>
              <li>
                <table>
                  <tbody>
                    <tr>
                      <td>KL divergence is asymmetric, i.e., $$D_{KL}(P</td>
                      <td>Â </td>
                      <td>Q)\ne D_{KL}(Q</td>
                      <td>Â </td>
                      <td>P)$$</td>
                    </tr>
                  </tbody>
                </table>
              </li>
            </ul>
          </li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>Application: i.e. æ‰¾åˆ°ä¸€distribution Qï¼Œä½¿å¾—å…¶ä¸true distribution Pæœ€ç›¸ç¬¦(ä¸”P,Qç›¸äº’ç‹¬ç«‹): $$\arg\min_{Q}D_{KL}(P</td>
                  <td>Â </td>
                  <td>Q)$$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>ä¸Šè¿°ç›®çš„ç­‰ä»·äº<script type="math/tex">\arg\min_{Q}H(P, Q)=\arg\min_{Q}-E_{x\sim P}\left[\log Q(x)\right]</script></li>
        </ul>
      </li>
      <li>Cross-Entropy can act as a Cost Function: log Loss/Cross-Entropy Loss
        <ul>
          <li>
            <script type="math/tex; mode=display">H(\text{p, q})=-\sum_{i}p_{i}\log(q_{i})</script>
          </li>
          <li>é‡‡ç”¨è‡ªç„¶åº•æ•°</li>
          <li>æ¢åº•å…¬å¼: <script type="math/tex">log_2(x)=log(x)/log(2)</script></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Index</th>
      <th>Weather</th>
      <th>Possibility</th>
      <th>Code</th>
      <th>Information</th>
      <th>Message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>ğŸŒ</td>
      <td>0.35</td>
      <td>000</td>
      <td>1.51 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>1</td>
      <td>â›…</td>
      <td>0.35</td>
      <td>001</td>
      <td>1.51 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>2</td>
      <td>â˜</td>
      <td>0.1</td>
      <td>010</td>
      <td>3.32 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>3</td>
      <td>ğŸŒ¨</td>
      <td>0.1</td>
      <td>011</td>
      <td>3.32 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>4</td>
      <td>ğŸŒ§</td>
      <td>0.04</td>
      <td>100</td>
      <td>4.64 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>5</td>
      <td>ğŸŒ©</td>
      <td>0.04</td>
      <td>101</td>
      <td>4.64 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>6</td>
      <td>ğŸŒª</td>
      <td>0.01</td>
      <td>110</td>
      <td>6.64 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>7</td>
      <td>ğŸŒ«</td>
      <td>0.01</td>
      <td>111</td>
      <td>6.64 bits</td>
      <td>3 bits</td>
    </tr>
  </tbody>
</table>

<p>è¡¨2</p>

<table>
  <thead>
    <tr>
      <th>Index</th>
      <th>Weather</th>
      <th>Possibility</th>
      <th>Code</th>
      <th>Information</th>
      <th>Message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>ğŸŒ</td>
      <td>0.35</td>
      <td>00</td>
      <td>1.51 bits</td>
      <td>2 bits</td>
    </tr>
    <tr>
      <td>1</td>
      <td>â›…</td>
      <td>0.35</td>
      <td>01</td>
      <td>1.51 bits</td>
      <td>2 bits</td>
    </tr>
    <tr>
      <td>2</td>
      <td>â˜</td>
      <td>0.1</td>
      <td>100</td>
      <td>3.32 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>3</td>
      <td>ğŸŒ¨</td>
      <td>0.1</td>
      <td>101</td>
      <td>3.32 bits</td>
      <td>3 bits</td>
    </tr>
    <tr>
      <td>4</td>
      <td>ğŸŒ§</td>
      <td>0.04</td>
      <td>1100</td>
      <td>4.64 bits</td>
      <td>4 bits</td>
    </tr>
    <tr>
      <td>5</td>
      <td>ğŸŒ©</td>
      <td>0.04</td>
      <td>1101</td>
      <td>4.64 bits</td>
      <td>4 bits</td>
    </tr>
    <tr>
      <td>6</td>
      <td>ğŸŒª</td>
      <td>0.01</td>
      <td>11100</td>
      <td>6.64 bits</td>
      <td>5 bits</td>
    </tr>
    <tr>
      <td>7</td>
      <td>ğŸŒ«</td>
      <td>0.01</td>
      <td>11101</td>
      <td>6.64 bits</td>
      <td>5 bits</td>
    </tr>
  </tbody>
</table>

<p>è¡¨3 (unambiguous code: prefix Huffman code)</p>

<table>
  <thead>
    <tr>
      <th>Code</th>
      <th>Bit</th>
      <th>Note</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>00</td>
      <td>2</td>
      <td>Â </td>
    </tr>
    <tr>
      <td>01</td>
      <td>2</td>
      <td>Â </td>
    </tr>
    <tr>
      <td>100</td>
      <td>3</td>
      <td>å‰2ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰2ä½</td>
    </tr>
    <tr>
      <td>101</td>
      <td>3</td>
      <td>Â </td>
    </tr>
    <tr>
      <td>1100</td>
      <td>4</td>
      <td>å‰3ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰3ä½</td>
    </tr>
    <tr>
      <td>1101</td>
      <td>4</td>
      <td>Â </td>
    </tr>
    <tr>
      <td>11100</td>
      <td>5</td>
      <td>å‰4ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰4ä½</td>
    </tr>
    <tr>
      <td>11101</td>
      <td>5</td>
      <td>Â </td>
    </tr>
  </tbody>
</table>

<p>è¡¨4 (å‰ç¼€ç¼–ç /prefix code)</p>

<table>
  <thead>
    <tr>
      <th>Â </th>
      <th>P(red)</th>
      <th>P(Blue)</th>
      <th>P(winning)</th>
      <th>-log2(P(winning))</th>
      <th>Entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ğŸ”´ğŸ”´ğŸ”´ğŸ”´</td>
      <td>1</td>
      <td>0</td>
      <td>1*1*1*1=1</td>
      <td>0+0+0+0=1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>ğŸ”´ğŸ”´ğŸ”´ğŸ”µ</td>
      <td>0.75</td>
      <td>0.25</td>
      <td>0.75*0.75*0.75*0.75=0.105</td>
      <td>0.415+0.415+0.415+2</td>
      <td>0.81</td>
    </tr>
    <tr>
      <td>ğŸ”´ğŸ”´ğŸ”µğŸ”µ</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.5*0.5*0.5*0.5=0.0625</td>
      <td>1+1+1+1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>è¡¨5</p>

<p><img src="https://image.slidesharecdn.com/featureselection-120719061056-phpapp02/95/feature-selection-14-638.jpg?cb=1389173732" alt="å›¾4" /></p>

<p><img src="https://cn.bing.com/th?id=OIP.g67516lIRLc0B93I7z5p-gHaFY&amp;pid=Api&amp;rs=1" alt="å›¾5" /></p>

<ul>
  <li>More details about KL divergence
    <ul>
      <li>Given P, we want to find Q* that minizes the KL divergence</li>
      <li>ç”±äºKL divergenceæ˜¯ä¸å¯¹ç§°çš„ï¼Œæœ‰ä¸¤ç§æœ€å°åŒ–</li>
      <li>å‚è§å›¾6</li>
    </ul>
  </li>
</ul>

<p><img src="../assets/img/Minimizer-of-KL-Divergence.png" alt="å›¾6" /></p>

<h3 id="reference">Reference</h3>

<ol>
  <li><a href="https://www.youtube.com/watch?v=ErfnhcEV1O8">ğŸ“º Entropy, Cross-Entropy and KL-Divergence</a></li>
  <li><a href="https://www.bilibili.com/video/av73032356?p=1">ğŸ“º Shannon Entropy and Information Gain</a></li>
  <li><a href="https://www.bilibili.com/video/av73032356?p=2">ğŸ“º Khan Academy, Shannon Entropy</a></li>
  <li><a href="https://www.bilibili.com/video/av73032356?p=4">ğŸ“º KL Divergence</a></li>
</ol>

<h3 id="application-in-multi-criteria-decision-making">Application in Multi-Criteria Decision-Making</h3>

<ul>
  <li>ç†µå€¼æ³•å¯ä»¥ä½œä¸ºä¸€ç§å®¢è§‚èµ‹æƒæ–¹æ³•</li>
  <li>è®¾æœ‰mä¸ªå¾…è¯„æ–¹æ¡ˆ(æ ·æœ¬Sample)ï¼Œné¡¹è¯„ä»·æŒ‡æ ‡(ç‰¹å¾Feature)ï¼Œå½¢æˆå…ƒç´ æŒ‡æ ‡æ•°æ®çŸ©é˜µ$X=(x_{ij})_{m\times n}$</li>
  <li>æ ¹æ®ç†µçš„ç‰¹æ€§ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—ç†µå€¼æ¥åˆ¤æ–­ä¸€ä¸ªæ–¹æ¡ˆçš„éšæœºæ€§åŠæ— åºç¨‹åº¦ï¼Œä¹Ÿå¯ä»¥ç”¨ç†µå€¼æ¥åˆ¤æ–­æŸæŒ‡æ ‡çš„ç¦»æ•£ç¨‹åº¦ï¼ŒæŒ‡æ ‡çš„ç¦»æ•£ç¨‹åº¦è¶Šå¤§ï¼Œè¯¥æŒ‡æ ‡å¯¹äºç»¼åˆè¯„ä»·çš„å½±å“è¶Šå¤§</li>
  <li>Data Preprocessing
    <ul>
      <li>é‡çº²ä¸å½±å“ï¼Œæ— éœ€æ ‡å‡†åŒ–</li>
      <li>æ•°æ®éè´ŸåŒ–å¤„ç†</li>
      <li>æ•°æ®å¹³ç§» (2ç§å¹³ç§»æ–¹å¼ï¼Œå‰è€…å¤„ç†è¶Šå¤§è¶Šå¥½çš„æ•°æ®ï¼Œåè€…å¤„ç†è¶Šå°è¶Šå¥½çš„æ•°æ®ï¼Œæœ€ç»ˆéƒ½æ˜¯è¶Šå¤§è¶Šå¥½)
        <ul>
          <li>
            <script type="math/tex; mode=display">x_{ij}=\cfrac{x_{ij}-\min(x_{1j},...,x_{mj})}{\max(x_{1j},...,x_{mj})-\min(x_{1j},...,x_{mj})}+1</script>
          </li>
          <li>
            <script type="math/tex; mode=display">x_{ij}=\cfrac{\max(x_{1j},...,x_{mj})-x_{ij}}{\max(x_{1j},...,x_{mj})-\min(x_{1j},...,x_{mj})}+1</script>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ç®—æ³•æ€è·¯: è®¡ç®—å‡ºæ¯åˆ—çš„Entropy, Entropyè¶Šå°çš„æƒé‡è¶Šå¤§
    <ul>
      <li>å…¶ä¸­possibility: $p_{ij}=\cfrac{x_{ij}}{\sum_{i}^{m}x_{ij}}$</li>
      <li>entropy: <script type="math/tex">e_{j}=-\sum_{i}^{m}p_{ij}\ln(p_{ij}) \rightarrow e_{j}=\cfrac{e_{j}}{\ln{m}} \Leftrightarrow e_{j}=-\cfrac{1}{\ln{m}}\sum_{i}^{m}p_{ij}\ln(p_{ij})</script>
        <ul>
          <li>å¤šä¸€ä¸ªç³»æ•°<script type="math/tex">1/\ln{m}</script>ä»£è¡¨ç€mç§æƒ…å†µéƒ½å‡ç­‰æ¦‚ç‡(é»˜è®¤æ¦‚ç‡å’Œå¿…ä¸º1)æ—¶çš„entropyå€¼ï¼Œæ­¤æ—¶çš„entropyæœ€å¤§(ç¦»æ•£ç¨‹åº¦æœ€ä½)ï¼Œæ­¤ç³»æ•°ä¿è¯è®¡ç®—å‡ºçš„å€¼åœ¨0-1å†…</li>
        </ul>
      </li>
      <li>å·®å¼‚ç³»æ•°: <script type="math/tex">g_{j}=1-e_{j}</script></li>
      <li>weight: <script type="math/tex">w_{j}=\cfrac{g_{j}}{\sum_{j}^{n}g_{j}}</script></li>
      <li>score: <script type="math/tex">s_{i}=\sum_{j}^{n}w_{j}*p_{ij}</script></li>
    </ul>
  </li>
</ul>

<p><a href="../blog.html">ğŸ’¨back</a></p>
:ET